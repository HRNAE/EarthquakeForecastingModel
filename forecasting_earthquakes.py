# -*- coding: utf-8 -*-
"""Forecasting Earthquakes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QeYvRUZFFjut5izThMzWcXLUhU1qPXRQ
"""

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import geopandas as gpd
from shapely.geometry import box
from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable
import numpy as np

# Load your dataset
df = pd.read_csv('/content/AA_DATA.csv')
print(df)
df.drop(0, inplace=True)
print(df.columns)

# Select the features for clustering (latitude and longitude)
X = df[['latitude', 'longitude']]

# Specify the number of clusters
n_clusters = 1
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# Visualize the clusters
plt.scatter(df['longitude'], df['latitude'], c=df['cluster'], cmap='viridis', marker='.')
plt.title('Earthquake Clusters')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['Rock Type'] = label_encoder.fit_transform(df['Rock Type'])
df['Domain'] = label_encoder.fit_transform(df['Domain'])
df['Fault Line'] = label_encoder.fit_transform(df['Fault Line'])

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Define the latitude and longitude ranges for the desired section
min_latitude, max_latitude = 27.5, 45
min_longitude, max_longitude = -140, -110

# Create a bounding box geometry for the desired section
bbox = box(minx=min_longitude, maxx=max_longitude, miny=min_latitude, maxy=max_latitude)

# Clip the world geometries to the bounding box
section = gpd.clip(world, bbox)

# Plot the desired section
fig, ax = plt.subplots(figsize=(10, 8))
section.plot(ax=ax, color='#d2b48c', edgecolor='#8b4513', alpha=0.8)

# Add grid and set aspect ratio
ax.grid(True, linestyle='--', linewidth=0.5)
ax.set_aspect('equal')
scatterplot = plt.scatter(df['longitude'], df['latitude'], c=df['cluster'], cmap='viridis', marker='.')

plt.title('Clusters of Earthquakes Visualized')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

import geopandas as gpd
import matplotlib.pyplot as plt

# Create a GeoDataFrame from the earthquake data
gdf_earthquakes = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))

# Plot the world map
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
fig, ax = plt.subplots(figsize=(12, 8))
section.plot(ax=ax, color='#d2b48c', edgecolor='#8b4513', alpha=0.8)

# Color-code magnitudes using the viridis colormap
norm = Normalize(vmin=gdf_earthquakes['mag'].min(), vmax=gdf_earthquakes['mag'].max())
cmap = plt.cm.viridis
sm = ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

# Plot earthquakes with color-coded magnitudes
gdf_earthquakes = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))
scatterplot = gdf_earthquakes.plot(ax=ax, marker='o', color=cmap(norm(gdf_earthquakes['mag'])), markersize=10, legend=True)

cbar = plt.colorbar(sm, orientation='vertical', label='Magnitude')

# Add grid and set aspect ratio
ax.grid(True, linestyle='--', linewidth=0.5)
ax.set_aspect('equal')

# Add legend and title
plt.legend(title='Magnitude', markerscale=0.5)
plt.title('Visualizing Earthquakes based on Magnitude', fontsize=16)
plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Latitude', fontsize=12)
plt.show()

# Create a dictionary to store DataFrames for each cluster
cluster_dataframes = {}

# Iterate over clusters and create DataFrames
for cluster_id in range(n_clusters):
    cluster_dataframes[cluster_id] = df[df['cluster'] == cluster_id].drop('cluster', axis=1)

print(cluster_dataframes)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer


label_encoder = LabelEncoder()
df['Rock Type'] = label_encoder.fit_transform(df['Rock Type'])
df['Domain'] = label_encoder.fit_transform(df['Domain'])
df['Fault Line'] = label_encoder.fit_transform(df['Fault Line'])

# Dictionary to store trained models for each cluster
cluster_models = {}
mse_scores = []
mae_scores = []
r2_scores = []
all_actual_magnitudes = []
all_predictions = []
x_test_latitude = []
x_test_longitude = []

# Loop through each cluster

cluster_df = cluster_dataframes[cluster_id]

# Sort the DataFrame by date or any relevant chronological column
cluster_df['time'] = pd.to_datetime(cluster_df['time'])

cluster_df['year'] = cluster_df['time'].dt.year
cluster_df['month'] = cluster_df['time'].dt.month
cluster_df['day'] = cluster_df['time'].dt.day
cluster_df['hour'] = cluster_df['time'].dt.hour
cluster_df['minute'] = cluster_df['time'].dt.minute
cluster_df['second'] = cluster_df['time'].dt.second

cluster_df = cluster_df.sort_values(by='time')  # Replace 'date_column' with your actual chronological column

# Split the data into training (80%) and testing (20%) sets
train_size = int(0.8 * len(cluster_df))
train_data, test_data = cluster_df[:train_size], cluster_df[train_size:]

# Separate features and target variables
X_train, y_train = train_data[['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'Domain',
    'Fault Line', 'Creep Rate' ]], train_data[['mag']]
X_test, y_test = test_data[['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'Domain',
    'Fault Line', 'Creep Rate' ]], test_data[['mag']]

for i in X_test['latitude']:
  x_test_latitude.append(i)
for i in X_test['longitude']:
  x_test_longitude.append(i)
y_train = y_train.values.ravel()

imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)  # Fit and transform for training data
X_test = imputer.transform(X_test)

# Initialize and train the RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=150, max_depth=7, random_state=42)
rf_model.fit(X_train, y_train)
# Make predictions on the test set
predictions = rf_model.predict(X_test)
print(predictions)

# Evaluate the model (you can use other metrics based on your specific needs)
mse = mean_squared_error(y_test, predictions)
mse_scores.append(mse)
mae = mean_absolute_error(y_test, predictions)
mae_scores.append(mae)

print(f"Cluster {cluster_id} Mean Squared Error: {mse}")
print(f"Cluster {cluster_id} Mean Absolute Error: {mae}")
print("")

actual_magnitudes = test_data['mag'].values

# Append actual magnitudes and predictions for each cluster
all_actual_magnitudes.extend(y_test.values.ravel())
all_predictions.extend(predictions)

df['time'] = pd.to_datetime(df['time'])
df['year'] = df['time'].dt.year
df['month'] = df['time'].dt.month
df['day'] = df['time'].dt.day
df['hour'] = df['time'].dt.hour
df['minute'] = df['time'].dt.minute
df['second'] = df['time'].dt.second

magnitude_threshold = 3.5  # Example threshold
df['earthquake_occurrence'] = (df['mag'] >= magnitude_threshold).astype(int)
X = df[['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'Domain','Fault Line', 'Creep Rate']]  # Drop non-feature and target columns
y = df['earthquake_occurrence']

imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)  # Fit and transform for training data

coordinates ={
    "Shore Road 1 XSH1" : [36.9430, -121.4450],
    "Pt Pinole CPP1" : [37.9900, -122.3560],
    "Temescal CTM1" : [37.8440, -122.2270],
    "Oakland Zoo COZ1" : [37.7530, -122.1500],
    "Hayward, Palisades St #2 CHP1" : [37.6630, -122.0740],
    "Fremont Winery CFW1" : [37.5320, -121.9520],
    "San Juan Bautista; Nyland SJN1": [36.8546, 0.0000],
    "San Juan Bautista #2 XSJ2": [36.8360, -121.5210],
    "San Juan Bautista #3 XSJ3": [36.8360, -121.5210],
    "Harris Ranch #1 XHR1": [36.7720, -121.4220],
    "Harris Ranch #2 XHR2": [36.7720, -121.4220],
    "Harris Ranch #3 XHR3": [36.7720, -121.4220],
    "Cienega Winery Central CWC3": [36.7500, -121.3850],
    "Cienega Winery North CWN1": [36.7500, -121.3850],
    "Melendy Ranch XMR1": [36.5950, -121.1870],
    "Slacks Canyon XSC1": [36.0650, -120.6280],
    "Middle Mtn XMM1": [35.9580, -120.5020],
    "Middle Mtn; Big Creep XMBC": [35.9580, -120.5020],
    "Middle Ridge XMD1": [35.9430, -120.4850],
    "Varian XVA1": [35.9220, -120.4620],
    "Roberson, SW trace XRSW": [35.9070, -120.4600],
    "Parkfield, #1 XPK1": [35.9020, -120.4420],
    "Parkfield, #2 XPK2": [35.9020, -120.4420],
    "Taylor Ranch XTA1": [35.8900, -120.4270],
    "Taylor Ranch; Big Creep TABC": [35.8900, -120.4270],
    "Hearst, SW trace XHSW": [35.8620, -120.4200],
    "Work Ranch WKR1": [35.8580, -120.3920],
    "Carr Ranch CRR1": [35.8350, -120.3630],
    "Gold Hill XGH1": [35.8200, -120.3480],
    "Highway 46 C461": [35.7240, -120.2820],
    "Highway 46 X461": [35.7230, -120.2780]
}

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

class_weights = {
    0: 1,   # Low magnitude
    1: 6.5,   # high magnitude
}



# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=150, max_depth = 7, random_state=42, class_weight= class_weights)
rf_classifier.fit(X_train, y_train)

# Predict probabilities for the test set
probabilities = rf_classifier.predict_proba(X_test)

# probabilities[:, 1] will give you the probabilities of earthquake occurrence
earthquake_probabilities = probabilities[:, 1]

# Example: print the first 5 predictions
print(earthquake_probabilities[:5])

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score

# Predictions (0 or 1) based on a default threshold of 0.5
predictions = rf_classifier.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, predictions)}")
print(f"Precision: {precision_score(y_test, predictions)}")
print(f"Recall: {recall_score(y_test, predictions)}")
print(f"ROC-AUC: {roc_auc_score(y_test, earthquake_probabilities)}")

domains = pd.read_csv("/content/CRM_polygonsNewData.csv")
domains

def creepRateFinder(time, fautline):
  df2Time = time
  faultLine = fautline
  creepRate = 0
  if faultLine == 'Shore Road 1 XSH1':
    for j in range(len(shoreRoad)):
      if(type(shoreRoad.iloc[j, 0]) == float):
          continue
      values = shoreRoad.iloc[j, 0].split()
      if df2Time[i] == values[0]:
        creepRate = values[2]
        break
      if j == len(shoreRoad) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Pt Pinole CPP1':
      for j in range(len(ptPinole)):
        if(type(ptPinole.iloc[j, 0]) == float):
          continue
        values = ptPinole.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(ptPinole) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'San Juan Bautista; Nyland SJN1' or faultLine == 'San Juan Bautista #2 XSJ2' or faultLine == 'San Juan Bautista #3 XSJ3':
      for j in range(len(sanJuan)):
        if(type(sanJuan.iloc[j, 0]) == float):
          continue
        values = sanJuan.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(sanJuan) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Harris Ranch #2 XHR2' or faultLine == 'Harris Ranch #3 XHR3':
      for j in range(len(harrisRanch)):
        if(type(harrisRanch.iloc[j, 0]) == float):
          continue
        values = harrisRanch.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(harrisRanch) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Cienega Winery Central CWC3' or faultLine == 'Cienega Winery North CWN1':
      for j in range(len(cienegaWinery)):
        if(type(cienegaWinery.iloc[j, 0]) == float):
          continue
        values = cienegaWinery.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(cienegaWinery) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Melendy Ranch XMR1':
      for j in range(len(melendyRanch)):
        if(type(melendyRanch.iloc[j, 0]) == float):
          continue
        values = melendyRanch.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(melendyRanch) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Slacks Canyon XSC1':
      for j in range(len(slacksCanyon)):
        if(type(slacksCanyon.iloc[j, 0]) == float):
          continue
        values = slacksCanyon.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(slacksCanyon) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Middle Mtn XMM1' or faultLine == 'Middle Mtn; Big Creep XMBC' or faultLine == 'Middle Ridge XMD1':
      for j in range(len(middleMtn)):
        if(type(middleMtn.iloc[j, 0]) == float):
          continue
        values = middleMtn.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(middleMtn) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Parkfield, #1 XPK1' or faultLine == 'Parkfield, #1 XPK2':
      for j in range(len(parkfield)):
        if(type(parkfield.iloc[j, 0]) == float):
          continue
        values = parkfield.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(parkfield) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Taylor Ranch XTA1' or faultLine == 'Taylor Ranch; Big Creep TABC':
      for j in range(len(taylorRanch)):
        if(type(taylorRanch.iloc[j, 0]) == float):
          continue
        values = taylorRanch.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(taylorRanch) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Hearst, SW trace XHSW':
      for j in range(len(hearstSW)):
        if(type(hearstSW.iloc[j, 0]) == float):
          continue
        values = hearstSW.iloc[j, 0].split()
        if df2Time[i] == values[0]:
         creepRate = values[2]
         break
        if j == len(hearstSW) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Work Ranch WKR1':
      for j in range(len(workRanch)):
        if(type(workRanch.iloc[j, 0]) == float):
          continue
        values = workRanch.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(workRanch) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Carr Ranch CRR1':
      for j in range(len(carrRanch)):
        if(type(carrRanch.iloc[j, 0]) == float):
          continue
        values = carrRanch.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(carrRanch) - 1:
          creepRate = None
          print('djfgihj')
  elif faultLine == 'Gold Hill XGH1':
      for j in range(len(goldHill)):
        if(type(goldHill.iloc[j, 0]) == float):
          continue
        values = goldHill.iloc[j, 0].split()
        if df2Time[i] == values[0]:
          creepRate = values[2]
          break
        if j == len(goldHill) - 1:
          creepRate = None
          print('djfgihj')
  return creepRate

import math
def forecast_input(year, month, day, hour, minute, second, lat, lon, ):
  distance = []
  for key, value in coordinates.items():
    x1 = value[1]
    y1 = value[0]
    distance.append(math.sqrt(pow(float(lon)-x1,2)+pow(float(lat)-y1,2)))
  nearest_fault = distance.index(min(distance))

  distance_domains = []
  for index, row in domains.iterrows():
    distance_domains.append(math.sqrt(pow(float(lon)-row['xcoord'],2)+pow(float(lat)-row['ycoord'],2)))
  nearest_domain = distance.index(min(distance))

  creep_rate = creepRateFinder(year, nearest_fault)

  user_input_data = [year, month, day, hour, minute, second, lat, lon, nearest_domain, nearest_fault, creep_rate]
  column_names = ['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'Domain', 'fault_name', 'Creep Rate']
  user_input_df = pd.DataFrame([user_input_data], columns=column_names)
  prediction = rf_classifier.predict(user_input_df)
  probability = rf_classifier.predict_proba(user_input_df)
  return probability[0]

user_input_lat = input("Enter Latitude: ")
user_input_long = input("Enter Longitude: ")
user_input_year = input("Enter Year: ")
user_input_month = input("Enter Month: ")
user_input_day = input("Enter Day: ")
user_input_hour = input("Enter Hour: ")
user_input_minute = input("Enter Minute: ")
user_input_second = input("Enter Second: ")

((forecast_input(user_input_year, user_input_month, user_input_day, user_input_hour, user_input_minute, user_input_second, user_input_lat, user_input_long)))

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Generate the confusion matrix
cm = confusion_matrix(y_test, predictions)
# Visualize the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Predicted Negative", "Predicted Positive"], yticklabels=["Actual Negative", "Actual Positive"])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

# Assuming df is already loaded as your DataFrame

# Create the binary target variable
df['target'] = (df['mag'] >= 3.5).astype(int)

# Selecting relevant features
X = df[['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'Rock Type', 'Domain', 'Quartz',
    'Feldspar', 'Mica', 'Pyroxene', 'Amphibole', 'Olivine', 'TOTAL', 'Creep Rate']]  # Simple imputation for missing values
y = df['target']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pip install lightgbm

import lightgbm as lgb

# Prepare the training and validation datasets
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# Set parameters
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'is_unbalance': 'true',  # because our data might be imbalanced
    'boosting': 'gbdt',
    'num_leaves': 50,
    'feature_fraction': 0.5,
    'bagging_fraction': 0.5,
    'bagging_freq': 20,
    'learning_rate': 0.05,
    'verbose': 0
}

# Train the model
num_round = 100
lgbm_model = lgb.train(params,
                       train_data,
                       num_boost_round=1000,
                       valid_sets=[train_data, valid_data])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Predicting
y_pred_prob = lgbm_model.predict(X_test)
y_pred = [1 if x >= 0.5 else 0 for x in y_pred_prob]  # Convert probabilities to binary output

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)
# Visualize the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Predicted Negative", "Predicted Positive"], yticklabels=["Actual Negative", "Actual Positive"])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

import pickle

# Save the model to a file
with open('your_model.pkl', 'wb') as model_file:
    pickle.dump(rf_model, model_file)



# Calculate the mean of the training target variable
mean_train_mag = y_train.mean()

# Create predictions based on the mean value
mean_predictions = np.full_like(predictions, fill_value=mean_train_mag)
# Evaluate the model using mean predictions
total_mse = mean_squared_error(all_actual_magnitudes, all_predictions)
total_mae = mean_absolute_error(all_actual_magnitudes, all_predictions)

print(f"Total Predictions Mean Squared Error: {total_mse}")
print(f"Total Predictions Mean Absolute Error: {total_mae}")

residuals = np.array(all_actual_magnitudes) - np.array(all_predictions)
plt.scatter(all_predictions, residuals, color='black', s=2)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.show()

rmse = np.sqrt(np.mean(residuals**2))
print(f"Root Mean Squared Error (RMSE): {rmse}")

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
magnitude_difference = [pred - actual for pred, actual in zip(all_predictions, all_actual_magnitudes)]

# Define the latitude and longitude ranges for the desired section
min_latitude, max_latitude = 27.5, 45
min_longitude, max_longitude = -140, -110

# Create a bounding box geometry for the desired section
bbox = box(minx=min_longitude, maxx=max_longitude, miny=min_latitude, maxy=max_latitude)

# Clip the world geometries to the bounding box
section = gpd.clip(world, bbox)

# Plot the desired section
fig, ax = plt.subplots(figsize=(10, 8))
section.plot(ax=ax, color='#d2b48c', edgecolor='#8b4513', alpha=0.8)

# Set up normalization
min_diff = min(magnitude_difference)
max_diff = max(magnitude_difference)
norm = Normalize(vmin=min_diff, vmax=max_diff)
cmap = plt.cm.viridis
sm = ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

# Add grid and set aspect ratio
ax.grid(True, linestyle='--', linewidth=0.5)
ax.set_aspect('equal')
scatterplot = plt.scatter(x_test_longitude, x_test_latitude, c=magnitude_difference, cmap='viridis', marker='.')
cbar = plt.colorbar(sm, orientation='vertical', label='Magnitude Difference')

plt.title('Difference of Magnitudes (Predicted - Actual)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

# Create histogram bins and counts
hist, bins = np.histogram(residuals, bins=10)

# Calculate bin centers
bin_centers = (bins[:-1] + bins[1:]) / 2

# Plot the bar graph
plt.bar(bin_centers, hist, width=(bins[1] - bins[0]), align='center', edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Generate synthetic dataset
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X.squeeze() + np.random.randn(100)  # True relationship with noise

# Train a Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=10, random_state=42)
rf_model.fit(X, y)

# Generate test data
X_test = np.linspace(0, 10, 100).reshape(-1, 1)

# Make predictions using the trained model
y_pred = rf_model.predict(X_test)

# Plot the true relationship and predictions
plt.figure(figsize=(10, 6))

# Plot true relationship
plt.scatter(X, y, label='True Relationship', color='blue')

# Plot individual trees' predictions
for tree_idx, tree in enumerate(rf_model.estimators_):
    y_tree_pred = tree.predict(X_test)
    plt.plot(X_test, y_tree_pred, linestyle='--', alpha=0.5, label=f'Tree {tree_idx}')

# Plot ensemble prediction
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Ensemble Prediction')

plt.title('Random Forest Regressor Illustration')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

# Assuming residuals is a variable holding the differences between predictions and actual values

# Define the threshold for closeness
threshold = 0.5

# Count residuals within the threshold
within_threshold_count = np.sum(np.abs(residuals) <= threshold)

# Calculate the percentage of residuals within the threshold
within_threshold_percentage = (within_threshold_count / len(residuals)) * 100

# Print the results
print(f"Number of residuals within {threshold} of the actual value: {within_threshold_count}")
print(f"Percentage of residuals within {threshold} of the actual value: {within_threshold_percentage:.3f}%")

import math
import geopandas as gpd
cmap = plt.cm.viridis
norm = Normalize(vmin=gdf_earthquakes['mag'].min(), vmax=gdf_earthquakes['mag'].max())


df_counties = pd.read_csv('/content/California Demographics - Sheet1 (2).csv')
df_counties['population'] = df_counties['population'].str.replace(',', '').astype(int)


df = df.rename(columns={'counties': 'county'})
print(df.columns)
df = pd.merge(df, df_counties, how='inner', on = 'county')
print(df.columns)

suceptibility_score = []

print(df.columns)

max_mag = (df['mag']).max()
max_cost = df['cost_x'].max()
max_pop = df['population_x'].max()
suceptibility_score = []

for index, row in df.iterrows():
    suceptibility_score.append((row['cost_x']/max_cost) * (row['population_x']/max_pop) * (100 * math.exp(row['mag'] - 9)))

print(suceptibility_score)
df['suceptibility_score'] = suceptibility_score

# Create a scatter plot
plt.figure(figsize=(10, 6))
sc = plt.scatter(df['longitude'], df['latitude'], c=df['suceptibility_score'], cmap='hot', alpha=0.5)

# Add a color bar
plt.colorbar(sc, label='Susceptibility Score')

# Set the plot titles and labels
plt.title('Earthquake Susceptibility Map')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Show the plot
plt.show()

import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize
from sklearn.preprocessing import MinMaxScaler, RobustScaler

# Load your data into a pandas DataFrame
# Winsorization to handle outliers by capping them at a specified percentile
winsorized_data = winsorize(df['suceptibility_score'], limits=[0, 0.05])  # Adjust limits if necessary

# Convert the winsorized data to a DataFrame column
df['winsorized_susceptibility'] = pd.Series(winsorized_data, index=df.index)

# Alternatively, you can trim the outliers based on a quantile threshold
upper_threshold = df['suceptibility_score'].quantile(0.95)  # 95th percentile
df_trimmed = df[df['suceptibility_score'] <= upper_threshold]

# Or use log transformation to reduce the effect of outliers (if data is positive)
df['log_susceptibility'] = np.log(df['suceptibility_score'] + 1)  # Adding 1 to handle zero values

# Or use RobustScaler from scikit-learn
scaler = RobustScaler()
df['robust_scaled_susceptibility'] = scaler.fit_transform(df[['suceptibility_score']])

# Now normalize the data using Min-Max scaling
min_max_scaler = MinMaxScaler()
df['normalized_susceptibility'] = min_max_scaler.fit_transform(df[['winsorized_susceptibility']])  # Use 'log_susceptibility' or 'robust_scaled_susceptibility' as needed

# The 'normalized_susceptibility' column is now scaled between 0 and 1
print(df['normalized_susceptibility'])

df.to_csv("/content/ALLDATA 2.csv", index=False)

import pandas as pd
df = pd.read_csv("/content/ALLDATA 2 (1).csv")
df['latitude'].values

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from mpl_toolkits.basemap import Basemap
from matplotlib.colors import LinearSegmentedColormap

# Sample data setup
# Assuming gdf is your GeoDataFrame with 'longitude', 'latitude', and 'susceptibility_score'

# Convert GeoDataFrame's coordinate system to match the Basemap's projection
# This would typically involve transforming the longitude and latitude to the map's projection,
# which can vary depending on the projection you choose for Basemap.

# For demonstration, let's assume x, y are the projected coordinates and 'susceptibility_score' is the score.
x, y = df['longitude'].values, df['latitude'].values
scores = df['normalized_susceptibility'].values

# Create a Basemap instance
fig, ax = plt.subplots(figsize=(10, 10))
m = Basemap(projection='merc', llcrnrlon=-125, llcrnrlat=32, urcrnrlon=-114, urcrnrlat=42, resolution='i')
m.drawcoastlines()
m.drawcountries()
m.fillcontinents(color='lightgray')
m.drawmapboundary(fill_color='aqua')

# Project x, y to map's coordinates
x_map, y_map = m(x, y)

# Calculate the KDE with susceptibility scores as weights
kde = gaussian_kde(np.vstack([x_map, y_map]), bw_method='silverman', weights=scores)

# Evaluate the KDE over a grid
xx, yy = np.mgrid[x_map.min():x_map.max():100j, y_map.min():y_map.max():100j]
grid_coords = np.vstack([xx.ravel(), yy.ravel()])
zz = np.reshape(kde(grid_coords), xx.shape)

# Plot the result as an overlay with a colormap
m.imshow(np.rot90(zz), extent=(x_map.min(), x_map.max(), y_map.min(), y_map.max()), cmap='hot', alpha=0.5)

# Add a colorbar and title
plt.colorbar(label='Density')
plt.title('Earthquake Susceptibility Map with KDE')

plt.show()

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from matplotlib.colors import LinearSegmentedColormap

df = pd.read_csv("/content/ALLDATA 2 (1).csv")
print(df.columns)

# Assuming 'df' is already loaded with your data
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.longitude, df.latitude)
)
# ...
# Initialize the figure with a black background
fig, ax = plt.subplots(figsize=(15, 15), facecolor='white')

# Create the Basemap with a black background
m = Basemap(projection='merc',
            llcrnrlat=gdf.geometry.y.min() - 1, urcrnrlat=gdf.geometry.y.max() + 1,
            llcrnrlon=gdf.geometry.x.min() - 1, urcrnrlon=gdf.geometry.x.max() + 1,
            lat_ts=20, resolution='i', ax=ax)

# Draw map boundaries and fill continents with a color that stands out on black
m.drawmapboundary(fill_color='aqua')
m.fillcontinents(color='beige', lake_color='aqua')

# Draw coastlines with a color that is visible on a black background
m.drawcoastlines(color='black')

# Convert the GeoDataFrame's coordinate system to the Basemap's coordinate system
x, y = m(gdf.geometry.x.values, gdf.geometry.y.values)

# Define a custom colormap for the gradient
cmap = LinearSegmentedColormap.from_list('mycmap', ['darkred', 'red', 'orange', 'yellow'])

# Use hexbin for a density plot, where the dots flow together
hb = m.hexbin(x, y, C=df['normalized_susceptibility'], gridsize=50, cmap=cmap, reduce_C_function=np.mean, bins=None)

# Add a colorbar with a suitable color for the text to be visible on a black background
cbar = plt.colorbar(hb, shrink=0.4)
cbar.set_label('Log10(N)', color='black')

# Set color of the ticks and the label of the colorbar to be visible on a black background
cbar.ax.yaxis.set_tick_params(color='black')
plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='black')

# Add a title with a color that is visible on a black background
plt.title('Earthquake Susceptibility Map of California', color='black')

# Show the plot
plt.show()

# To save the figure as an image file with a black background
# fig.savefig('earthquake_susceptibility_map.png', facecolor=fig.get_facecolor(), edgecolor='none')